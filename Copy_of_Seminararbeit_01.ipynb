{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JonasMiksch/Research_Project_SimCLR/blob/main/Copy_of_Seminararbeit_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYpg9zU0PzC1",
        "outputId": "6d24d902-2396-45eb-dd08-e55613756651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.11-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 21.8 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |▊                               | 40 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 71 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 81 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 92 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 112 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 122 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 133 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 143 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 153 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 163 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 174 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 184 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 194 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 204 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████                            | 215 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 225 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 235 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 245 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 256 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 266 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████                           | 276 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 286 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 296 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 307 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 317 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████                          | 327 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 337 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 348 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 358 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 368 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████                         | 378 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 389 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 399 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 409 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 419 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 430 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████                        | 440 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 450 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 460 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 471 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 481 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 491 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 501 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 512 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 522 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 532 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 542 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 552 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 563 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 573 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 583 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 593 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 604 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 614 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 624 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 634 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 645 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 655 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 665 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 675 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 686 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 696 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 706 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 716 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 727 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 737 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 747 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 757 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 768 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 778 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 788 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 798 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 808 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 819 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 829 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 839 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 849 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 860 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 870 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 880 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 890 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 901 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 911 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 921 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 931 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 942 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 952 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 962 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 972 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 983 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 993 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.0 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.0 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.0 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.0 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.0 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.1 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.1 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.1 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.1 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.1 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.1 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.1 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.1 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.2 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.2 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.2 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.2 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.2 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.2 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.2 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.2 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.2 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.3 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.3 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.3 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.3 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.3 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.3 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.3 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.3 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.3 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.4 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.4 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.4 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.4 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.4 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.4 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.4 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.4 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.4 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.4 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.5 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.5 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.5 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.5 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.5 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.5 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.5 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.5 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.5 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.5 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.6 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.6 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.6 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.6 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.6 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.6 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.6 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.6 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.6 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.6 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.7 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.7 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.7 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.7 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.7 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.7 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.7 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.7 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.7 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.7 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.6-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 47.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.2-cp37-cp37m-manylinux1_x86_64.whl (36 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 46.9 MB/s \n",
            "\u001b[?25hCollecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=94e1e46e7b5fbfd4664f81705119839b2bd116ba5cc8530799fabed874f9d93c\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, yaspin, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.6 setproctitle-1.2.2 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.11 yaspin-2.1.0\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.6.3-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.6.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Global seed set to 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "from copy import deepcopy\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "plt.set_cmap('cividis')\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "## tqdm for loading bars\n",
        "from tqdm.notebook import tqdm\n",
        "#from torchvision.datasets import STL10\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "from math import sqrt, ceil, floor\n",
        "from statistics import mean\n",
        "\n",
        "from torchvision.io import read_image\n",
        "from random import randint\n",
        "\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torchvision.utils import make_grid\n",
        "from torch.nn import functional as F\n",
        "import torch\n",
        "\n",
        "try:\n",
        "    import wandb\n",
        "except ModuleNotFoundError: \n",
        "    !pip3 install wandb\n",
        "    import wandb\n",
        "\n",
        "try:\n",
        "    import torchinfo\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip3 install torchinfo\n",
        "    import torchinfo\n",
        "\n",
        "## Torchvision\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "import numpy as np\n",
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip3 install --quiet pytorch-lightning>=1.4 # type : ignore\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "import pandas as pd\n",
        "import pdb\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "# Import tensorboard\n",
        "%reload_ext tensorboard\n",
        "\n",
        "\n",
        "# In this notebook, we use data loaders with heavier computational processing. It is recommended to use as many\n",
        "# workers as possible in a data loader, which corresponds to the number of CPU cores\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "# Setting the seed\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.determinstic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jX2CcrUvGg14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtcgdxUam4rp"
      },
      "source": [
        "**Load Image Data **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4HQf0tkqWp2",
        "outputId": "9c763650-bde4-4117-89db-9be0dde2c579"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "\n",
        "!unzip gdrive/My\\ Drive/Seminararbeit/JPEGImages.zip > /dev/null\n",
        "\n",
        "\n",
        "DATASET_PATH = '/content/gdrive/My Drive/Seminararbeit'\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH =  '/content/gdrive/My Drive/Seminararbeit/checkpoints'\n",
        "\n",
        "img_folder = 'JPEGImages'\n",
        "\n",
        "\n",
        "#STOP COLLAB FROM DISCONNECTING\n",
        "\n",
        "# function ConnectButton(){\n",
        "#     console.log(\"Connect pushed\"); \n",
        "#     document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n",
        "# }\n",
        "\n",
        "# var colab = setInterval(ConnectButton,60000);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXSlcU_Nm8D9"
      },
      "source": [
        "**Define Lightning Module for Style Transfer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgGd9B4lP_ln"
      },
      "outputs": [],
      "source": [
        "class MSEContentLoss(nn.Module):\n",
        "    # https://arxiv.org/pdf/1508.06576.pdf\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        return F.mse_loss(x, y)\n",
        "\n",
        "\n",
        "class GramStyleLoss(nn.Module):\n",
        "    # https://arxiv.org/pdf/1508.06576.pdf\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        gram_diff = self.gram_matrix(x) - self.gram_matrix(y)\n",
        "        return torch.mean(torch.sum(gram_diff ** 2, dim=[1, 2]))\n",
        "\n",
        "    def gram_matrix(self, x):\n",
        "        n, c, h, w = x.size()\n",
        "        x = x.view(n, c, h * w)\n",
        "        return x @ x.transpose(-2, -1) / (c * h * w)\n",
        "\n",
        "\n",
        "class MomentMatchingStyleLoss(nn.Module):\n",
        "    # https://arxiv.org/pdf/1703.06868.pdf\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x_mean = torch.mean(x, dim=[2, 3])\n",
        "        y_mean = torch.mean(y, dim=[2, 3])\n",
        "        mean_loss = F.mse_loss(x_mean, y_mean)\n",
        "\n",
        "        x_std = torch.std(x, dim=[2, 3])\n",
        "        y_std = torch.std(y, dim=[2, 3])\n",
        "        std_loss = F.mse_loss(x_std, y_std)\n",
        "\n",
        "        return mean_loss + std_loss\n",
        "\n",
        "\n",
        "class CMDStyleLoss(nn.Module):\n",
        "    # https://arxiv.org/pdf/2103.07208.pdf\n",
        "    # CMDStyleLoss works with pre-activation outputs of VGG19 (without ReLU)\n",
        "\n",
        "    def __init__(self, k=5):\n",
        "        super().__init__()\n",
        "        self.k = k\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x, y = torch.sigmoid(x), torch.sigmoid(y)\n",
        "\n",
        "        loss = 0\n",
        "        for x_k, y_k in zip(self.moments(x), self.moments(y)):\n",
        "            loss += self.l2_dist(x_k, y_k).mean()\n",
        "        return loss\n",
        "\n",
        "    def moments(self, x):\n",
        "        # First vectorize feature maps\n",
        "        n, c, h, w = x.size()\n",
        "        x = x.view(n, c, h * w)\n",
        "\n",
        "        x_mean = torch.mean(x, dim=2, keepdim=True)\n",
        "        x_centered = x - x_mean\n",
        "\n",
        "        moments = [x_mean.squeeze(-1)]\n",
        "        for n in range(2, self.k + 1):\n",
        "            moments.append(torch.mean(x_centered ** n, dim=2))\n",
        "        return moments\n",
        "\n",
        "    def l2_dist(self, x, y):\n",
        "        return torch.norm(x - y, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJ5hK5_0QD5l"
      },
      "outputs": [],
      "source": [
        "\n",
        "class AdaConv2d(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, n_groups=None):\n",
        "        super().__init__()\n",
        "        self.n_groups = in_channels if n_groups is None else n_groups\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        padding = (kernel_size - 1) / 2\n",
        "        self.conv = nn.Conv2d(in_channels=in_channels,\n",
        "                              out_channels=out_channels,\n",
        "                              kernel_size=(kernel_size, kernel_size),\n",
        "                              padding=(ceil(padding), floor(padding)),\n",
        "                              padding_mode='reflect')\n",
        "\n",
        "    def forward(self, x, w_spatial, w_pointwise, bias):\n",
        "        assert len(x) == len(w_spatial) == len(w_pointwise) == len(bias)\n",
        "        x = F.instance_norm(x)\n",
        "\n",
        "        # F.conv2d does not work with batched filters (as far as I can tell)...\n",
        "        # Hack for inputs with > 1 sample\n",
        "        ys = []\n",
        "        for i in range(len(x)):\n",
        "            y = self._forward_single(x[i:i + 1], w_spatial[i], w_pointwise[i], bias[i])\n",
        "            ys.append(y)\n",
        "        ys = torch.cat(ys, dim=0)\n",
        "\n",
        "        ys = self.conv(ys)\n",
        "        return ys\n",
        "\n",
        "    def _forward_single(self, x, w_spatial, w_pointwise, bias):\n",
        "        # Only square kernels\n",
        "        assert w_spatial.size(-1) == w_spatial.size(-2)\n",
        "        padding = (w_spatial.size(-1) - 1) / 2\n",
        "        pad = (ceil(padding), floor(padding), ceil(padding), floor(padding))\n",
        "\n",
        "        x = F.pad(x, pad=pad, mode='reflect')\n",
        "        x = F.conv2d(x, w_spatial, groups=self.n_groups)\n",
        "        x = F.conv2d(x, w_pointwise, groups=self.n_groups, bias=bias)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TecrjNnQDyU"
      },
      "outputs": [],
      "source": [
        "class KernelPredictor(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, n_groups, style_channels, kernel_size):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.w_channels = style_channels\n",
        "        self.n_groups = n_groups\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        padding = (kernel_size - 1) / 2\n",
        "        self.spatial = nn.Conv2d(style_channels,\n",
        "                                 in_channels * out_channels // n_groups,\n",
        "                                 kernel_size=kernel_size,\n",
        "                                 padding=(ceil(padding), ceil(padding)),\n",
        "                                 padding_mode='reflect')\n",
        "        self.pointwise = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Conv2d(style_channels,\n",
        "                      out_channels * out_channels // n_groups,\n",
        "                      kernel_size=1)\n",
        "        )\n",
        "        self.bias = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Conv2d(style_channels,\n",
        "                      out_channels,\n",
        "                      kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, w):\n",
        "        w_spatial = self.spatial(w)\n",
        "        w_spatial = w_spatial.reshape(len(w),\n",
        "                                      self.out_channels,\n",
        "                                      self.in_channels // self.n_groups,\n",
        "                                      self.kernel_size, self.kernel_size)\n",
        "\n",
        "        w_pointwise = self.pointwise(w)\n",
        "        w_pointwise = w_pointwise.reshape(len(w),\n",
        "                                          self.out_channels,\n",
        "                                          self.out_channels // self.n_groups,\n",
        "                                          1, 1)\n",
        "\n",
        "        bias = self.bias(w)\n",
        "        bias = bias.reshape(len(w),\n",
        "                            self.out_channels)\n",
        "\n",
        "        return w_spatial, w_pointwise, bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1-VHO1DQDqd"
      },
      "outputs": [],
      "source": [
        "class VGGEncoder(nn.Module):\n",
        "    def __init__(self, normalize=True, post_activation=True):\n",
        "        super().__init__()\n",
        "\n",
        "        if normalize:\n",
        "            mean = [0.485, 0.456, 0.406]\n",
        "            std = [0.229, 0.224, 0.225]\n",
        "            self.normalize = transforms.Normalize(mean=mean, std=std)\n",
        "        else:\n",
        "            self.normalize = nn.Identity()\n",
        "\n",
        "        if post_activation:\n",
        "            layer_names = {'relu1_1', 'relu2_1', 'relu3_1', 'relu4_1'}\n",
        "        else:\n",
        "            layer_names = {'conv1_1', 'conv2_1', 'conv3_1', 'conv4_1'}\n",
        "        blocks, block_names, scale_factor, out_channels = extract_vgg_blocks(models.vgg19(pretrained=True).features,\n",
        "                                                                             layer_names)\n",
        "\n",
        "        self.blocks = nn.ModuleList(blocks)\n",
        "        self.block_names = block_names\n",
        "        self.scale_factor = scale_factor\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "    def forward(self, xs):\n",
        "        xs = self.normalize(xs)\n",
        "\n",
        "        features = []\n",
        "        for block in self.blocks:\n",
        "            xs = block(xs)\n",
        "            features.append(xs)\n",
        "\n",
        "        return features\n",
        "\n",
        "    def freeze(self):\n",
        "        self.eval()\n",
        "        for parameter in self.parameters():\n",
        "            parameter.requires_grad = False\n",
        "\n",
        "\n",
        "\n",
        "def extract_vgg_blocks(layers, layer_names):\n",
        "    blocks, current_block, block_names = [], [], []\n",
        "    scale_factor, out_channels = -1, -1\n",
        "    depth_idx, relu_idx, conv_idx = 1, 1, 1\n",
        "    for layer in layers:\n",
        "        name = ''\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            name = f'conv{depth_idx}_{conv_idx}'\n",
        "            current_out_channels = layer.out_channels\n",
        "            layer.padding_mode = 'reflect'\n",
        "            conv_idx += 1\n",
        "        elif isinstance(layer, nn.ReLU):\n",
        "            name = f'relu{depth_idx}_{relu_idx}'\n",
        "            layer = nn.ReLU(inplace=False)\n",
        "            relu_idx += 1\n",
        "        elif isinstance(layer, nn.AvgPool2d) or isinstance(layer, nn.MaxPool2d):\n",
        "            name = f'pool{depth_idx}'\n",
        "            depth_idx += 1\n",
        "            conv_idx = 1\n",
        "            relu_idx = 1\n",
        "        else:\n",
        "            warnings.warn(f' Unexpected layer type: {type(layer)}')\n",
        "\n",
        "        current_block.append(layer)\n",
        "        if name in layer_names:\n",
        "            blocks.append(nn.Sequential(*current_block))\n",
        "            block_names.append(name)\n",
        "            scale_factor = 1 * 2 ** (depth_idx - 1)\n",
        "            out_channels = current_out_channels\n",
        "            current_block = []\n",
        "\n",
        "    return blocks, block_names, scale_factor, out_channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYcRrJvuQDcA"
      },
      "outputs": [],
      "source": [
        "class AdaConvModel(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, style_size, style_channels, kernel_size):\n",
        "        super().__init__()\n",
        "        self.encoder = VGGEncoder()\n",
        "\n",
        "        style_in_shape = (self.encoder.out_channels, style_size // self.encoder.scale_factor, style_size // self.encoder.scale_factor)\n",
        "        style_out_shape = (style_channels, kernel_size, kernel_size)\n",
        "        self.style_encoder = GlobalStyleEncoder(in_shape=style_in_shape, out_shape=style_out_shape)\n",
        "        self.decoder = AdaConvDecoder(style_channels=style_channels, kernel_size=kernel_size)\n",
        "\n",
        "    def forward(self, content, style, return_embeddings=False):\n",
        "        self.encoder.freeze()\n",
        "\n",
        "        # Encode -> Decode\n",
        "        content_embeddings, style_embeddings = self._encode(content, style)\n",
        "        output = self._decode(content_embeddings[-1], style_embeddings[-1])\n",
        "\n",
        "        # Return embeddings if training\n",
        "        if return_embeddings:\n",
        "            output_embeddings = self.encoder(output)\n",
        "            embeddings = {\n",
        "                'content': content_embeddings,\n",
        "                'style': style_embeddings,\n",
        "                'output': output_embeddings\n",
        "            }\n",
        "            return output, embeddings\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def _encode(self, content, style):\n",
        "        content_embeddings = self.encoder(content)\n",
        "        style_embeddings = self.encoder(style)\n",
        "        return content_embeddings, style_embeddings\n",
        "\n",
        "    def _decode(self, content_embedding, style_embedding):\n",
        "        style_embedding = self.style_encoder(style_embedding)\n",
        "        output = self.decoder(content_embedding, style_embedding)\n",
        "        return output\n",
        "\n",
        "\n",
        "class AdaConvDecoder(nn.Module):\n",
        "    def __init__(self, style_channels, kernel_size):\n",
        "        super().__init__()\n",
        "        self.style_channels = style_channels\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        # Inverted VGG with first conv in each scale replaced with AdaConv\n",
        "        group_div = [1, 2, 4, 8]\n",
        "        n_convs = [1, 4, 2, 2]\n",
        "        self.layers = nn.ModuleList([\n",
        "            *self._make_layers(512, 256, group_div=group_div[0], n_convs=n_convs[0]),\n",
        "            *self._make_layers(256, 128, group_div=group_div[1], n_convs=n_convs[1]),\n",
        "            *self._make_layers(128, 64, group_div=group_div[2], n_convs=n_convs[2]),\n",
        "            *self._make_layers(64, 3, group_div=group_div[3], n_convs=n_convs[3], final_act=False, upsample=False)])\n",
        "\n",
        "    def forward(self, content, w_style):\n",
        "        # Checking types is a bit hacky, but it works well.\n",
        "        for module in self.layers:\n",
        "            if isinstance(module, KernelPredictor):\n",
        "                w_spatial, w_pointwise, bias = module(w_style)\n",
        "            elif isinstance(module, AdaConv2d):\n",
        "                content = module(content, w_spatial, w_pointwise, bias)\n",
        "            else:\n",
        "                content = module(content)\n",
        "        return content\n",
        "\n",
        "    def _make_layers(self, in_channels, out_channels, group_div, n_convs, final_act=True, upsample=True):\n",
        "        n_groups = in_channels // group_div\n",
        "\n",
        "        layers = []\n",
        "        for i in range(n_convs):\n",
        "            last = i == n_convs - 1\n",
        "            out_channels_ = out_channels if last else in_channels\n",
        "            if i == 0:\n",
        "                layers += [\n",
        "                    KernelPredictor(in_channels, in_channels,\n",
        "                                    n_groups=n_groups,\n",
        "                                    style_channels=self.style_channels,\n",
        "                                    kernel_size=self.kernel_size),\n",
        "                    AdaConv2d(in_channels, out_channels_, n_groups=n_groups)]\n",
        "            else:\n",
        "                layers.append(nn.Conv2d(in_channels, out_channels_, 3,\n",
        "                                        padding=1, padding_mode='reflect'))\n",
        "\n",
        "            if not last or final_act:\n",
        "                layers.append(nn.ReLU())\n",
        "\n",
        "        if upsample:\n",
        "            layers.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
        "        return layers\n",
        "\n",
        "\n",
        "class GlobalStyleEncoder(nn.Module):\n",
        "    def __init__(self, in_shape, out_shape):\n",
        "        super().__init__()\n",
        "        self.in_shape = in_shape\n",
        "        self.out_shape = out_shape\n",
        "        channels = in_shape[0]\n",
        "\n",
        "        self.downscale = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels, 3, padding=1, padding_mode='reflect'),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.AvgPool2d(2, 2),\n",
        "            #\n",
        "            nn.Conv2d(channels, channels, 3, padding=1, padding_mode='reflect'),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.AvgPool2d(2, 2),\n",
        "            #\n",
        "            nn.Conv2d(channels, channels, 3, padding=1, padding_mode='reflect'),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.AvgPool2d(2, 2),\n",
        "        )\n",
        "\n",
        "        in_features = self.in_shape[0] * (self.in_shape[1] // 8) * self.in_shape[2] // 8\n",
        "        out_features = self.out_shape[0] * self.out_shape[1] * self.out_shape[2]\n",
        "        self.fc = nn.Linear(in_features, out_features)\n",
        "\n",
        "    def forward(self, xs):\n",
        "        ys = self.downscale(xs)\n",
        "        ys = ys.reshape(len(xs), -1)\n",
        "\n",
        "        w = self.fc(ys)\n",
        "        w = w.reshape(len(xs), self.out_shape[0], self.out_shape[1], self.out_shape[2])\n",
        "        return w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifZQ3W-qQWa_"
      },
      "outputs": [],
      "source": [
        "class LightningModel(pl.LightningModule):\n",
        "\n",
        "    def __init__(self,\n",
        "                 model_type,\n",
        "                 alpha,\n",
        "                 style_size, style_channels, kernel_size,\n",
        "                 style_loss, style_weight,\n",
        "                 content_loss, content_weight,\n",
        "                 lr, lr_decay,\n",
        "                 **_):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.lr = lr\n",
        "        self.lr_decay = lr_decay\n",
        "        self.style_weight = style_weight\n",
        "        self.content_weight = content_weight\n",
        "\n",
        "        # Style loss\n",
        "        if style_loss == 'mm':\n",
        "            self.style_loss = MomentMatchingStyleLoss()\n",
        "        elif style_loss == 'gram':\n",
        "            self.style_loss = GramStyleLoss()\n",
        "        elif style_loss == 'cmd':\n",
        "            self.style_loss = CMDStyleLoss()\n",
        "        else:\n",
        "            raise ValueError('style_loss')\n",
        "\n",
        "        # Content loss\n",
        "        if content_loss == 'mse':\n",
        "            self.content_loss = MSEContentLoss()\n",
        "        else:\n",
        "            raise ValueError('content_loss')\n",
        "\n",
        "        # Model type\n",
        "        #if model_type == 'adain':\n",
        "            #self.model = AdaINModel(alpha)\n",
        "        #elif model_type == 'adaconv':\n",
        "        if model_type == 'adaconv':\n",
        "            self.model = AdaConvModel(style_size, style_channels, kernel_size)\n",
        "        else:\n",
        "            raise ValueError('model_type')\n",
        "\n",
        "    def forward(self, content, style, return_embeddings=False):\n",
        "        return self.model(content, style, return_embeddings)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self.shared_step(batch, 'train')\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return self.shared_step(batch, 'val')\n",
        "\n",
        "    def shared_step(self, batch, step):\n",
        "        content, style = batch['content'], batch['style']\n",
        "        output, embeddings = self.model(content, style, return_embeddings=True)\n",
        "        content_loss, style_loss = self.loss(embeddings)\n",
        "\n",
        "        # Log metrics\n",
        "        self.log(rf'{step}/loss_style', style_loss.item(), prog_bar=step == 'train')\n",
        "        self.log(rf'{step}/loss_content', content_loss.item(), prog_bar=step == 'train')\n",
        "\n",
        "        # Return output only for validation step\n",
        "        if step == 'val':\n",
        "            return {\n",
        "                'loss': content_loss + style_loss,\n",
        "                'output': output,\n",
        "            }\n",
        "        return content_loss + style_loss\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        if self.global_step == 0:\n",
        "            return\n",
        "\n",
        "        with torch.no_grad():\n",
        "            imgs = [x['output'] for x in outputs]\n",
        "            imgs = [img for triple in imgs for img in triple]\n",
        "            nrow = int(sqrt(len(imgs)))\n",
        "            grid = make_grid(imgs, nrow=nrow, padding=0)\n",
        "            logger = self.logger.experiment\n",
        "            logger.add_image(rf'val_img', grid, global_step=self.global_step + 1)\n",
        "\n",
        "    def loss(self, embeddings):\n",
        "        # Content\n",
        "        content_loss = self.content_loss(embeddings['content'][-1], embeddings['output'][-1])\n",
        "\n",
        "        # Style\n",
        "        style_loss = []\n",
        "        for (style_features, output_features) in zip(embeddings['style'], embeddings['output']):\n",
        "            style_loss.append(self.style_loss(style_features, output_features))\n",
        "        style_loss = sum(style_loss)\n",
        "\n",
        "        return self.content_weight * content_loss, self.style_weight * style_loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = Adam(self.parameters(), lr=self.lr)\n",
        "\n",
        "        def lr_lambda(iter):\n",
        "            return 1 / (1 + 0.0002 * iter)\n",
        "\n",
        "        lr_scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
        "\n",
        "        return {\n",
        "            'optimizer': optimizer,\n",
        "            'lr_scheduler': {\n",
        "                \"scheduler\": lr_scheduler,\n",
        "                \"interval\": \"step\",\n",
        "                \"frequency\": 1,\n",
        "            },\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-l47ZZPUVVb"
      },
      "outputs": [],
      "source": [
        "class ContrastiveTransformations(object):\n",
        "\n",
        "    def __init__(self, base_transforms, n_views=2):\n",
        "        self.base_transforms = base_transforms\n",
        "        self.n_views = n_views\n",
        "\n",
        "    #get 2 random transformations\n",
        "    def __call__(self, x):\n",
        "        return [self.base_transforms(x) for i in range(self.n_views)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvKnpPzm2Lmy"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Stylize(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.model_style = LightningModel.load_from_checkpoint(checkpoint_path=os.path.join(CHECKPOINT_PATH,\"model.ckpt\"))\n",
        "        #self.model_style = self.model_style.to(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
        "        self.model_style = self.model_style.to(torch.device( \"cpu\"))\n",
        "        self.model_style.eval()\n",
        "        self.how_often = 0\n",
        "    def __call__(self, sample):\n",
        "        image,style1,style2 = sample['content'], sample['style1'],sample['style2']\n",
        "        if(self.how_often %2 == 0):\n",
        "          style = style1\n",
        "        else:\n",
        "          style = style2\n",
        "        self.how_often +=1\n",
        "        with torch.no_grad():\n",
        "          device = next(self.model_style.parameters()).device\n",
        "          #pdb.set_trace()\n",
        "          #get tensors to gpu\n",
        "          content = image.unsqueeze(0)\n",
        "          style = style.unsqueeze(0)\n",
        "          output = self.model_style(content, style)\n",
        "          output =  output[0]\n",
        "\n",
        "          return output\n",
        "          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "e4a650cbeb0e4838b64ddd1fe2de9ae1",
            "e6bee66c99834895ad71996839fe256b",
            "85854ae49a014d9481138548ef9e3c6f",
            "37dde61973e146ad807be028852fcbea",
            "b0222637d5614cec8c3c2480d7b6d96d",
            "7900ab38aec545d183e106d2774d9c54",
            "103a948028da4d35a93e5e84bb7f7fd4",
            "c1fcb371bc0149aaacd68ec077f9c027",
            "8cbc246349794c17b09d420cfbdc8a2a",
            "9b81369f88d64dd5be493d1813b71cd7",
            "73fa1b7b03b34c8da3d822c89369cae7"
          ]
        },
        "id": "j7OLivEYzDMU",
        "outputId": "5067ad00-286c-4d61-ca13-7d95d0ecb3e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4a650cbeb0e4838b64ddd1fe2de9ae1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "content_transform = transforms.Compose([transforms.ToPILImage(),\n",
        "                                          transforms.Resize((256,256)),\n",
        "                                          transforms.ToTensor(),\n",
        "                                         ])\n",
        "style_transform = transforms.Compose([transforms.ToPILImage(),\n",
        "                                          transforms.Resize(256),\n",
        "                                          transforms.CenterCrop(size=(256,256)),\n",
        "                                          transforms.ToTensor(),\n",
        "                                         ])\n",
        "style_transfer = transforms.Compose([Stylize(),\n",
        "                                     transforms.Normalize((0.5,), (0.5,))\n",
        "                                         ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMbk2PNiKFFq"
      },
      "outputs": [],
      "source": [
        "from random import randint\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "  def __init__(self,csv,img_folder,transform):\n",
        "    self.csv=csv\n",
        "    self.transform=transform\n",
        "    self.img_folder=img_folder\n",
        "    \n",
        "    self.image_names=self.csv[:]['item']\n",
        "    self.labels=np.array(self.csv.drop(['item', 'set', 'artdl_label'], axis=1))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_names)\n",
        "\n",
        "  \n",
        "  def __getitem__(self,index):\n",
        "    img_path = os.path.join(self.img_folder, self.image_names.iloc[index]+'.jpg')\n",
        "    image = read_image(img_path)\n",
        "\n",
        "    original = content_transform(image)\n",
        "    index2 = -1\n",
        "    index3 = -1\n",
        "    #dont get the same image for style images\n",
        "    while index2 == index or index3 == index or index3 == index2 or index2 == -1:\n",
        "      index2 = randint(0,self.labels.size-1)\n",
        "      index3 = randint(0,self.labels.size-1)\n",
        "\n",
        "    styleImage_path = os.path.join(self.img_folder, self.image_names.iloc[index2]+'.jpg')\n",
        "    styleImage = style_transform(read_image(styleImage_path))\n",
        "\n",
        "    styleImage_path2 = os.path.join(self.img_folder, self.image_names.iloc[index3]+'.jpg')\n",
        "    styleImage2 = style_transform(read_image(styleImage_path2))\n",
        "\n",
        "    sample = {'content': original, 'style1': styleImage, 'style2': styleImage2}\n",
        "    image = self.transform(sample)\n",
        "    return image,self.labels[index]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaE0H56QU2dp"
      },
      "outputs": [],
      "source": [
        "#Load ImageDataset\n",
        "\n",
        "train_data = pd.read_csv(os.path.join(DATASET_PATH,'artdl_train.csv'))\n",
        "test_data = pd.read_csv(os.path.join(DATASET_PATH,'artdl_test.csv'))\n",
        "val_data = pd.read_csv(os.path.join(DATASET_PATH,'artdl_valid.csv'))\n",
        "\n",
        "train_data_contrast=ImageDataset(train_data,img_folder,ContrastiveTransformations(style_transfer, n_views=2))\n",
        "unlabeled_data=ImageDataset(test_data,img_folder,ContrastiveTransformations(style_transfer, n_views=2))\n",
        "val_data_contrast=ImageDataset(val_data,img_folder,ContrastiveTransformations(style_transfer, n_views=2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4OYWFtUnKzc"
      },
      "source": [
        "**Display Example Style Transfers on Test Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCyUzBRFU6Rn"
      },
      "outputs": [],
      "source": [
        "# Visualize some examples\n",
        "#ONLY FOR VISUALISATION\n",
        "# pl.seed_everything(42)\n",
        "# NUM_IMAGES = 6\n",
        "# imgs = torch.stack([img for idx in range(NUM_IMAGES) for img in unlabeled_data[idx][0]], dim=0)\n",
        "# img_grid = torchvision.utils.make_grid(imgs, nrow=6, normalize=True, pad_value=0.9)\n",
        "# img_grid = img_grid.permute(1, 2, 0)\n",
        "# plt.figure(figsize=(10,5))\n",
        "# plt.title('Augmented image examples of the ArtDL dataset')\n",
        "# plt.imshow(img_grid)\n",
        "# plt.axis('off')\n",
        "# plt.show()\n",
        "# plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXY2HlubnSWe"
      },
      "source": [
        "**Pretraining the SimCLR-net**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8TCCOFWVWEB"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SimCLR(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, hidden_dim, lr, temperature, weight_decay, max_epochs=50):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters('hidden_dim','lr','temperature','weight_decay',\"max_epochs\")\n",
        "        assert self.hparams.temperature > 0.0, 'The temperature must be a positive float!'\n",
        "        # Base model f(.)\n",
        "        self.convnet = torchvision.models.resnet50(pretrained=False,\n",
        "                                                   num_classes=4*hidden_dim)  # Output of last linear layer\n",
        "        # The MLP for g(.) consists of Linear->ReLU->Linear\n",
        "        self.convnet.fc = nn.Sequential(\n",
        "            self.convnet.fc,  # Linear(ResNet output, 4*hidden_dim)\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4*hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(self.parameters(),\n",
        "                                lr=self.hparams.lr,\n",
        "                                weight_decay=self.hparams.weight_decay)\n",
        "        lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
        "                                                            T_max=self.hparams.max_epochs,\n",
        "                                                            eta_min=self.hparams.lr/50)\n",
        "        return [optimizer], [lr_scheduler]\n",
        "\n",
        "    def info_nce_loss(self, batch, mode='train'):\n",
        "        imgs, _ = batch\n",
        "        imgs = torch.cat(imgs, dim=0)\n",
        "\n",
        "        # Encode all images\n",
        "        feats = self.convnet(imgs)\n",
        "        # Calculate cosine similarity\n",
        "        cos_sim = F.cosine_similarity(feats[:,None,:], feats[None,:,:], dim=-1)\n",
        "        # Mask out cosine similarity to itself\n",
        "        self_mask = torch.eye(cos_sim.shape[0], dtype=torch.bool, device=cos_sim.device)\n",
        "        cos_sim.masked_fill_(self_mask, -9e15)\n",
        "        # Find positive example -> batch_size//2 away from the original example\n",
        "        pos_mask = self_mask.roll(shifts=cos_sim.shape[0]//2, dims=0)\n",
        "        # InfoNCE loss\n",
        "        cos_sim = cos_sim / self.hparams.temperature\n",
        "        nll = -cos_sim[pos_mask] + torch.logsumexp(cos_sim, dim=-1)\n",
        "        nll = nll.mean()\n",
        "\n",
        "        # Logging loss\n",
        "        #self.log(mode+'_loss', nll)\n",
        "        # Get ranking position of positive example\n",
        "        comb_sim = torch.cat([cos_sim[pos_mask][:,None],  # First position positive example\n",
        "                              cos_sim.masked_fill(pos_mask, -9e15)],\n",
        "                             dim=-1)\n",
        "        sim_argsort = comb_sim.argsort(dim=-1, descending=True).argmin(dim=-1)\n",
        "        # Logging ranking metrics\n",
        "        #wandb.log({mode+' accuracy_top1': (sim_argsort == 0).float().mean(),\n",
        "                   #mode+' accuracy_top5': (sim_argsort <5).float().mean(),\n",
        "                   #mode+ ' loss': nll})\n",
        "        return nll\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self.info_nce_loss(batch, mode='train')\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        self.info_nce_loss(batch, mode='val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_FG7DiUvlmc"
      },
      "outputs": [],
      "source": [
        "    train_loader = data.DataLoader(train_data_contrast, batch_size=128, shuffle=True,\n",
        "                                       drop_last=True, pin_memory=True, num_workers=2)\n",
        "    val_loader = data.DataLoader(val_data_contrast, batch_size=128, shuffle=False,\n",
        "                                     drop_last=False, pin_memory=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDVQ3kkgVvOK"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def train_simclr(resume,batch_size, max_epochs, **kwargs,):\n",
        "    #wandb.init(project=\"SimCLR_Standard\",resume=allow,id = \"SimCLR_Test\")\n",
        "    train_loader = data.DataLoader(train_data_contrast, batch_size=batch_size, shuffle=True,\n",
        "                                       drop_last=True, pin_memory=True, num_workers=0)\n",
        "    val_loader = data.DataLoader(val_data_contrast, batch_size=batch_size, shuffle=False,\n",
        "                                     drop_last=False, pin_memory=True, num_workers=0)\n",
        "    \n",
        "    #Resume Training\n",
        "    if os.path.exists(os.path.join(CHECKPOINT_PATH,\"last_checkpoint_ArtDl\"))and resume == False:\n",
        "        print(\"load finished model\")\n",
        "        model = SimCLR.load_from_checkpoint(checkpoint_path=os.path.join(CHECKPOINT_PATH,\"last_checkpoint_ArtDl-ArtDl\"))\n",
        "        return model\n",
        "\n",
        "    if os.path.exists(os.path.join(CHECKPOINT_PATH,\"last_checkpoint_ArtDl\"))and resume:\n",
        "        #wandb.init(project=\"SimCLRl\",resume=\"allow\",id = \"ArtDL10\")\n",
        "        print(\"resume_training\")\n",
        "        #checkpoint_callback = ModelCheckpoint(dirpath=CHECKPOINT,filename = \"last_checkpoint.ckpt\")\n",
        "\n",
        "        trainer = pl.Trainer(resume_from_checkpoint=os.path.join(CHECKPOINT_PATH,\"last_checkpoint_ArtDl\"),\n",
        "                             default_root_dir='/content/gdrive/My Drive/Seminararbeit/trainer',\n",
        "                             gpus=1 if str(device)=='cuda:0' else 0,\n",
        "                             max_epochs=max_epochs+10,\n",
        "                             #callbacks=[checkpoint_callback],\n",
        "                             progress_bar_refresh_rate=1)\n",
        "        \n",
        "        model = SimCLR.load_from_checkpoint(checkpoint_path=os.path.join(CHECKPOINT_PATH,\"last_checkpoint_ArtDl-ArtDl\"))\n",
        "\n",
        "    #First Training Step\n",
        "    else:\n",
        "        print(\"start_training\")\n",
        "        #wandb.init(project=\"SimCLR\",resume=\"allow\",id = \"ArtDL10\")\n",
        "        #checkpoint_callback = ModelCheckpoint(dirpath=CHECKPOINT,filename = \"last_checkpoint.ckpt\")\n",
        "        trainer = pl.Trainer(default_root_dir='/content/gdrive/My Drive/Seminararbeit/trainer',\n",
        "                         gpus=1 if str(device)=='cuda:0' else 0,\n",
        "                         max_epochs=max_epochs,\n",
        "                         #callbacks=[checkpoint_callback],\n",
        "                         progress_bar_refresh_rate=1)\n",
        "        model = SimCLR(max_epochs=max_epochs, **kwargs)\n",
        "    pl.seed_everything(42) # To be reproducable\n",
        "\n",
        "    trainer.fit(model, train_loader, val_loader)\n",
        "    trainer.save_checkpoint(os.path.join(CHECKPOINT_PATH,\"last_checkpoint_ArtDl-ArtDl\"))\n",
        "    #trainer.save_checkpoint(checkpoint_path=os.path.join(CHECKPOINT,\"last_checkpoint.ckpt\"))\n",
        "    model = SimCLR.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814,
          "referenced_widgets": [
            "1cb666b27a714b1cb0e8fd34a3798756",
            "c0492c61ad794526a3d773e4c86cfd48",
            "7316c260e8544e4daf0a6f4bbb3d1433",
            "792ae8483300465aa6364f10fa558332",
            "f5fb672888b641268cebdb9b15340610",
            "ec63bd26b5954ae6be1c15fab5214802",
            "9ce3445fe88546df89176837a509c783",
            "af4e302f177b4c39a27889762e311b52",
            "8ba3f0d2f92145be977e973086aa5415",
            "d013d101ca60489986b3425e2248f4ea",
            "a9178692e1304c89996abd917f610a9b",
            "8568f6ef52a247aa9ad63d57a5f58eb8",
            "e786861fd09249f3a1eebc366346e906",
            "e3507d0ce5c34f1f908e140d9885bb00",
            "dd5e650d9fa543c7a68bd17fbba730b9",
            "2ec9171ca785471d84c83c6ec1c7b90c",
            "6f4e826095ad40fcbfa860967252efce",
            "1302d6233e17418d83e01071096547dd",
            "59e485b8af98457d88ff746a2a39aefa",
            "f8d7004065c74fd6a3576fd5e33c8fdc",
            "9b531092a84048e69a01eea078d8dedf",
            "f725c30730c54abf994d47dd6fb27df5"
          ]
        },
        "id": "20-Fc5C2WDjS",
        "outputId": "ed673736-c9e3-4f1f-8870-0a60e85ffca1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=1)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
            "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start_training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Global seed set to 42\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name    | Type   | Params\n",
            "-----------------------------------\n",
            "0 | convnet | ResNet | 24.6 M\n",
            "-----------------------------------\n",
            "24.6 M    Trainable params\n",
            "0         Non-trainable params\n",
            "24.6 M    Total params\n",
            "98.491    Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1cb666b27a714b1cb0e8fd34a3798756",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Validation sanity check: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Global seed set to 42\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8568f6ef52a247aa9ad63d57a5f58eb8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-0207edcb43cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                             \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.07\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                             \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                             max_epochs=8)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-56a028e273f8>\u001b[0m in \u001b[0;36mtrain_simclr\u001b[0;34m(resume, batch_size, max_epochs, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# To be reproducable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHECKPOINT_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"last_checkpoint_ArtDl-ArtDl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m#trainer.save_checkpoint(checkpoint_path=os.path.join(CHECKPOINT,\"last_checkpoint.ckpt\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0mtrain_dataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         self._call_and_handle_interrupt(\n\u001b[0;32m--> 741\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m         )\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \"\"\"\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0;31m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;31m# TODO: ckpt_path only in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0mckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt_path\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m         \u001b[0;31m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1277\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_predicting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1279\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;31m# double dispatch to initiate the training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_evaluating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_training_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_EVALUATE_OUTPUT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;31m# the global step is manually decreased here due to backwards compatibility with existing loggers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_batch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrement_processed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/batch/training_batch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautomatic_optimization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0moptimizers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_active_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_frequencies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, batch, *args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         )\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001b[0m in \u001b[0;36m_run_optimization\u001b[0;34m(self, split_batch, batch_idx, optimizer, opt_idx)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;31m# gradient update with accumulated gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsume_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001b[0m in \u001b[0;36m_optimizer_step\u001b[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mon_tpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mDeviceType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_TPU_AVAILABLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0musing_native_amp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp_backend\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp_backend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mAMPType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNATIVE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0musing_lbfgs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_lbfgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m         )\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/lightning.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m         \"\"\"\n\u001b[0;32m-> 1652\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_closure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptimizer_zero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofiler_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \"\"\"\n\u001b[1;32m    338\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptimizer_zero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_epoch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/precision/precision_plugin.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, model, optimizer, optimizer_idx, closure, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mclosure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_closure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_track_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/precision/precision_plugin.py\u001b[0m in \u001b[0;36m_wrap_closure\u001b[0;34m(self, model, optimizer, optimizer_idx, closure)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mconsistent\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mPrecisionPlugin\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0msubclasses\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mcannot\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdirectly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \"\"\"\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mclosure_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mclosure_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mClosureResult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_profiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training_step_and_backward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mstep_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001b[0m in \u001b[0;36m_training_step\u001b[0;34m(self, split_batch, batch_idx, opt_idx)\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_fx_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"training_step\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training_step\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                 \u001b[0mtraining_step_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, step_kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \"\"\"\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpost_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpost_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-91af6e605a0b>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo_nce_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-91af6e605a0b>\u001b[0m in \u001b[0;36minfo_nce_loss\u001b[0;34m(self, batch, mode)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Encode all images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Calculate cosine similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mcos_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mbn_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mexponential_average_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         )\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2282\u001b[0m     return torch.batch_norm(\n\u001b[0;32m-> 2283\u001b[0;31m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2284\u001b[0m     )\n\u001b[1;32m   2285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 11.17 GiB total capacity; 10.47 GiB already allocated; 9.81 MiB free; 10.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "#Adjust resume parameter \n",
        "simclr_model = train_simclr(resume = True,\n",
        "                            batch_size=128,\n",
        "                            hidden_dim=128,\n",
        "                            lr=5e-4,\n",
        "                            temperature=0.07,\n",
        "                            weight_decay=1e-4,\n",
        "                            max_epochs=8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zNRog0pnb6J"
      },
      "source": [
        "**Training of the classifier - not yet adapted**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bO0bqDe7WHxa"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, feature_dim, num_classes, lr, weight_decay, max_epochs=100):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        # Mapping from representation h to classes\n",
        "        self.model = nn.Linear(feature_dim, num_classes)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(self.parameters(),\n",
        "                                lr=self.hparams.lr,\n",
        "                                weight_decay=self.hparams.weight_decay)\n",
        "        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                                      milestones=[int(self.hparams.max_epochs*0.6),\n",
        "                                                                  int(self.hparams.max_epochs*0.8)],\n",
        "                                                      gamma=0.1)\n",
        "        return [optimizer], [lr_scheduler]\n",
        "\n",
        "    def _calculate_loss(self, batch, mode='train'):\n",
        "        feats, labels = batch\n",
        "        preds = self.model(feats)\n",
        "        loss = F.cross_entropy(preds, labels)\n",
        "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
        "\n",
        "        #self.log(mode + '_loss', loss)\n",
        "        #self.log(mode + '_acc', acc)\n",
        "\n",
        "        wandb.log({mode + '_acc': acc, mode + '_loss': loss})\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self._calculate_loss(batch, mode='train')\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        self._calculate_loss(batch, mode='val')\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        self._calculate_loss(batch, mode='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WZmxfvsY1OV"
      },
      "outputs": [],
      "source": [
        "#DIFFERENT TRANSFORMS\n",
        "#transforms.ToPILImage(),\n",
        "img_transforms = transforms.Compose([transforms.ToPILImage(),\n",
        "                                     transforms.Resize(256),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5,), (0.5,),\n",
        "                                                      )])\n",
        "\n",
        "train_data_class=ImageDataset(train_data,img_folder,img_transforms)\n",
        "test_data_class=ImageDataset(test_data,img_folder,img_transforms)\n",
        "\n",
        "#train_img_data = STL10(root=DATASET_PATH, split=\"train\", download=True, transform=img_transforms)\n",
        "#test_img_data = STL10(root=DATASET_PATH, split=\"test\", download=True, transform=img_transforms)\n",
        "\n",
        "print(\"Number of training examples:\", len(train_data_class))\n",
        "print(\"Number of test examples:\", len(test_data_class))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzQI-hNeY1K_"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def prepare_data_features(model, dataset):\n",
        "    # Prepare model\n",
        "    network = deepcopy(model.convnet)\n",
        "    network.fc = nn.Identity()  # Removing projection head g(.)\n",
        "    network.eval()\n",
        "    network.to(device)\n",
        "    print(\"x2\")\n",
        "    # Encode all images\n",
        "    data_loader = data.DataLoader(dataset, batch_size=64, num_workers=NUM_WORKERS, shuffle=False, drop_last=False)\n",
        "    feats, labels = [], []\n",
        "    print(\"xy\")\n",
        "    for batch_imgs, batch_labels in tqdm(data_loader):        #error here!!\n",
        "        print(\"x1\")\n",
        "        batch_imgs = batch_imgs.to(device)\n",
        "        batch_feats = network(batch_imgs)\n",
        "        feats.append(batch_feats.detach().cpu())\n",
        "        labels.append(batch_labels)\n",
        "\n",
        "    feats = torch.cat(feats, dim=0)\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "\n",
        "    # Sort images by labels\n",
        "    labels, idxs = labels.sort()\n",
        "    feats = feats[idxs]\n",
        "\n",
        "    return data.TensorDataset(feats, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIshIDYdaWrA"
      },
      "outputs": [],
      "source": [
        "train_feats_simclr = prepare_data_features(simclr_model, train_data_class)\n",
        "test_feats_simclr = prepare_data_features(simclr_model, test_data_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6X4beJr5YlSJ"
      },
      "outputs": [],
      "source": [
        "def train_logreg(batch_size, train_feats_data, test_feats_data, model_suffix, max_epochs=100, **kwargs):\n",
        "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"LogisticRegression\"),\n",
        "                         gpus=1 if str(device)==\"cuda:0\" else 0,\n",
        "                         max_epochs=max_epochs,\n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode='max', monitor='val_acc'),\n",
        "                                    LearningRateMonitor(\"epoch\")],\n",
        "                         progress_bar_refresh_rate=0,\n",
        "                         check_val_every_n_epoch=10)\n",
        "    trainer.logger._default_hp_metric = None\n",
        "\n",
        "    # Data loaders\n",
        "    train_loader = data.DataLoader(train_feats_data, batch_size=batch_size, shuffle=True,\n",
        "                                   drop_last=False, pin_memory=True, num_workers=0)\n",
        "    test_loader = data.DataLoader(test_feats_data, batch_size=batch_size, shuffle=False,\n",
        "                                  drop_last=False, pin_memory=True, num_workers=0)\n",
        "\n",
        "    # Check whether pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"LogisticRegression_{model_suffix}.ckpt\")\n",
        "    # if os.path.isfile(pretrained_filename):\n",
        "    #     print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
        "    #     model = LogisticRegression.load_from_checkpoint(pretrained_filename)\n",
        "    # else:\n",
        "    pl.seed_everything(42)  # To be reproducable\n",
        "    model = LogisticRegression(**kwargs)\n",
        "    trainer.fit(model, train_loader, test_loader)\n",
        "    model = LogisticRegression.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
        "\n",
        "    # Test best model on train and validation set\n",
        "    train_result = trainer.test(model, test_dataloaders=train_loader, verbose=False)\n",
        "    test_result = trainer.test(model, test_dataloaders=test_loader, verbose=False)\n",
        "    result = {\"train\": train_result[0][\"test_acc\"], \"test\": test_result[0][\"test_acc\"]}\n",
        "\n",
        "    return model, result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXB6-i6Mahlp"
      },
      "outputs": [],
      "source": [
        "def get_smaller_dataset(original_dataset, num_imgs_per_label):\n",
        "    new_dataset = data.TensorDataset(\n",
        "        *[t.unflatten(0, (10, -1))[:,:num_imgs_per_label].flatten(0, 1) for t in original_dataset.tensors]\n",
        "    )\n",
        "    return new_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jK7LDHviafZu"
      },
      "outputs": [],
      "source": [
        "results = {}\n",
        "for num_imgs_per_label in [10, 20, 50, 100, 200, 500]:\n",
        "    sub_train_set = get_smaller_dataset(train_feats_simclr, num_imgs_per_label)\n",
        "    _, small_set_results = train_logreg(batch_size=64,\n",
        "                                        train_feats_data=sub_train_set,\n",
        "                                        test_feats_data=test_feats_simclr,\n",
        "                                        model_suffix=num_imgs_per_label,\n",
        "                                        feature_dim=train_feats_simclr.tensors[0].shape[1],\n",
        "                                        num_classes=10,\n",
        "                                        lr=1e-3,\n",
        "                                        weight_decay=1e-3)\n",
        "    results[num_imgs_per_label] = small_set_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQkrEX9TYlXL"
      },
      "outputs": [],
      "source": [
        "dataset_sizes = sorted([k for k in results])\n",
        "test_scores = [results[k][\"test\"] for k in dataset_sizes]\n",
        "\n",
        "fig = plt.figure(figsize=(6,4))\n",
        "plt.plot(dataset_sizes, test_scores, '--', color=\"#000\", marker=\"*\", markeredgecolor=\"#000\", markerfacecolor=\"y\", markersize=16)\n",
        "plt.xscale(\"log\")\n",
        "plt.xticks(dataset_sizes, labels=dataset_sizes)\n",
        "plt.title(\"STL10 classification over dataset size\", fontsize=14)\n",
        "plt.xlabel(\"Number of images per class\")\n",
        "plt.ylabel(\"Test accuracy\")\n",
        "plt.minorticks_off()\n",
        "plt.show()\n",
        "\n",
        "for k, score in zip(dataset_sizes, test_scores):\n",
        "    print(f'Test accuracy for {k:3d} images per label: {100*score:4.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FKdEP0wkxnD"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of Seminararbeit_01.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e4a650cbeb0e4838b64ddd1fe2de9ae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e6bee66c99834895ad71996839fe256b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_85854ae49a014d9481138548ef9e3c6f",
              "IPY_MODEL_37dde61973e146ad807be028852fcbea",
              "IPY_MODEL_b0222637d5614cec8c3c2480d7b6d96d"
            ]
          }
        },
        "e6bee66c99834895ad71996839fe256b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "85854ae49a014d9481138548ef9e3c6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7900ab38aec545d183e106d2774d9c54",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_103a948028da4d35a93e5e84bb7f7fd4"
          }
        },
        "37dde61973e146ad807be028852fcbea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c1fcb371bc0149aaacd68ec077f9c027",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 574673361,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 574673361,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8cbc246349794c17b09d420cfbdc8a2a"
          }
        },
        "b0222637d5614cec8c3c2480d7b6d96d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9b81369f88d64dd5be493d1813b71cd7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 548M/548M [00:07&lt;00:00, 62.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_73fa1b7b03b34c8da3d822c89369cae7"
          }
        },
        "7900ab38aec545d183e106d2774d9c54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "103a948028da4d35a93e5e84bb7f7fd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c1fcb371bc0149aaacd68ec077f9c027": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8cbc246349794c17b09d420cfbdc8a2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9b81369f88d64dd5be493d1813b71cd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "73fa1b7b03b34c8da3d822c89369cae7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1cb666b27a714b1cb0e8fd34a3798756": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c0492c61ad794526a3d773e4c86cfd48",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7316c260e8544e4daf0a6f4bbb3d1433",
              "IPY_MODEL_792ae8483300465aa6364f10fa558332",
              "IPY_MODEL_f5fb672888b641268cebdb9b15340610"
            ]
          }
        },
        "c0492c61ad794526a3d773e4c86cfd48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "7316c260e8544e4daf0a6f4bbb3d1433": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ec63bd26b5954ae6be1c15fab5214802",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Validation sanity check: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9ce3445fe88546df89176837a509c783"
          }
        },
        "792ae8483300465aa6364f10fa558332": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_af4e302f177b4c39a27889762e311b52",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8ba3f0d2f92145be977e973086aa5415"
          }
        },
        "f5fb672888b641268cebdb9b15340610": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d013d101ca60489986b3425e2248f4ea",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/2 [31:57&lt;00:00, 958.15s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a9178692e1304c89996abd917f610a9b"
          }
        },
        "ec63bd26b5954ae6be1c15fab5214802": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9ce3445fe88546df89176837a509c783": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "af4e302f177b4c39a27889762e311b52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8ba3f0d2f92145be977e973086aa5415": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d013d101ca60489986b3425e2248f4ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a9178692e1304c89996abd917f610a9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8568f6ef52a247aa9ad63d57a5f58eb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e786861fd09249f3a1eebc366346e906",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e3507d0ce5c34f1f908e140d9885bb00",
              "IPY_MODEL_dd5e650d9fa543c7a68bd17fbba730b9",
              "IPY_MODEL_2ec9171ca785471d84c83c6ec1c7b90c"
            ]
          }
        },
        "e786861fd09249f3a1eebc366346e906": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "e3507d0ce5c34f1f908e140d9885bb00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6f4e826095ad40fcbfa860967252efce",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Epoch 0:   0%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1302d6233e17418d83e01071096547dd"
          }
        },
        "dd5e650d9fa543c7a68bd17fbba730b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_59e485b8af98457d88ff746a2a39aefa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 132,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f8d7004065c74fd6a3576fd5e33c8fdc"
          }
        },
        "2ec9171ca785471d84c83c6ec1c7b90c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9b531092a84048e69a01eea078d8dedf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/132 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f725c30730c54abf994d47dd6fb27df5"
          }
        },
        "6f4e826095ad40fcbfa860967252efce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1302d6233e17418d83e01071096547dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "59e485b8af98457d88ff746a2a39aefa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f8d7004065c74fd6a3576fd5e33c8fdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9b531092a84048e69a01eea078d8dedf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f725c30730c54abf994d47dd6fb27df5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}